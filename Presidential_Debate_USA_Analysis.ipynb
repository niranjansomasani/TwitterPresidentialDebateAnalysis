{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USA Presidential Debate Analysis - Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import tweepy # to access through twitter API\n",
    "import pandas as pd # To import and export data using different data formats\n",
    "import numpy as np # to manipulate data\n",
    "import json # to import json files\n",
    "import csv # to access CSV files\n",
    "# For plotting and visualization:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion and import\n",
    "Data imported was in json so converting it into CSV and using those for further analysis. Next few lines helps us achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for line in open('C:\\\\Users\\\\NIRANJAN SOMASANI\\\\Downloads\\\\FirstPresidentialDebateTweets', 'r', encoding='utf-8'):\n",
    "    tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_out = open('C:\\\\Users\\\\NIRANJAN SOMASANI\\\\Downloads\\\\FirstPresidentialDebateTweets2.csv', 'w', encoding='utf-8') #opens csv file\n",
    "writer = csv.writer(csv_out) #create the csv writer object\n",
    "\n",
    "fields = ['created_at', 'text', 'screen_name', 'followers', 'Likes', 'friends', 'rt'] #field names\n",
    "writer.writerow(fields) #writes field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in tweets:\n",
    " \n",
    "    #writes a row and gets the fields from the json object\n",
    "    #screen_name and followers/friends are found on the second level hence two get methods\n",
    "    writer.writerow([line.get('created_at'),\n",
    "                     line.get('full_text').encode('unicode_escape'), #unicode escape to fix emoji issue\n",
    "                     line.get('user').get('screen_name'),\n",
    "                     line.get('user').get('followers_count'),\n",
    "                     line.get('user').get('favourites_count'),\n",
    "                     line.get('user').get('friends_count'),\n",
    "                     line.get('retweet_count')])\n",
    " \n",
    "csv_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for line in open('C:\\\\Users\\\\NIRANJAN SOMASANI\\\\Downloads\\\\SecondPresidentialDebateTweets', 'r', encoding='utf-8'):\n",
    "    tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_out = open('C:\\\\Users\\\\NIRANJAN SOMASANI\\\\Downloads\\\\SecondPresidentialDebateTweets2.csv', 'w', encoding='utf-8') #opens csv file\n",
    "writer = csv.writer(csv_out) #create the csv writer object\n",
    "\n",
    "fields = ['created_at', 'text', 'screen_name', 'followers', 'Likes', 'friends', 'rt'] #field names\n",
    "writer.writerow(fields) #writes field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in tweets:\n",
    " \n",
    "    #writes a row and gets the fields from the json object\n",
    "    #screen_name and followers/friends are found on the second level hence two get methods\n",
    "    writer.writerow([line.get('created_at'),\n",
    "                     line.get('full_text').encode('unicode_escape'), #unicode escape to fix emoji issue\n",
    "                     line.get('user').get('screen_name'),\n",
    "                     line.get('user').get('followers_count'),\n",
    "                     line.get('user').get('favourites_count'),\n",
    "                     line.get('user').get('friends_count'),\n",
    "                     line.get('retweet_count')])\n",
    " \n",
    "csv_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for line in open('C:\\\\Users\\\\NIRANJAN SOMASANI\\\\Downloads\\\\ThirdPresidentialDebateTweets', 'r', encoding='utf-8'):\n",
    "    tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_out = open('C:\\\\Users\\\\NIRANJAN SOMASANI\\\\Downloads\\\\ThirdPresidentialDebateTweets2.csv', 'w', encoding='utf-8') #opens csv file\n",
    "writer = csv.writer(csv_out) #create the csv writer object\n",
    "\n",
    "fields = ['created_at', 'text', 'screen_name', 'followers', 'Likes', 'friends', 'rt'] #field names\n",
    "writer.writerow(fields) #writes field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in tweets:\n",
    " \n",
    "    #writes a row and gets the fields from the json object\n",
    "    #screen_name and followers/friends are found on the second level hence two get methods\n",
    "    writer.writerow([line.get('created_at'),\n",
    "                     line.get('full_text').encode('unicode_escape'), #unicode escape to fix emoji issue\n",
    "                     line.get('user').get('screen_name'),\n",
    "                     line.get('user').get('followers_count'),\n",
    "                     line.get('user').get('favourites_count'),\n",
    "                     line.get('user').get('friends_count'),\n",
    "                     line.get('retweet_count')])\n",
    " \n",
    "csv_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first = pd.read_csv('C:\\\\Users\\\\NIRANJAN SOMASANI\\\\Downloads\\\\FirstPresidentialDebateTweets2.csv')\n",
    "second = pd.read_csv('C:\\\\Users\\\\NIRANJAN SOMASANI\\\\Downloads\\\\SecondPresidentialDebateTweets2.csv')\n",
    "third = pd.read_csv('C:\\\\Users\\\\NIRANJAN SOMASANI\\\\Downloads\\\\ThirdPresidentialDebateTweets2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "I'll analyse each debate sorting it by time in which the tweets are put on the day of the debate to the next day. \n",
    "I want to see the tweets trend and support over the time to see how the debate influenced the support for the presidential candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first.info())\n",
    "print(first.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(second.info())\n",
    "print(second.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(third.info())\n",
    "print(third.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first.isnull().any())\n",
    "print(second.isnull().any())\n",
    "print(third.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first.head()\n",
    "#first[first['screen_name'] == 'TomHall']\n",
    "#first['screen_name'].value_counts().sort_values(ascending = False)\n",
    "#highest_num_of_tweets = np.max(data['account_name'].value_counts().sort_values(ascending = False))\n",
    "#highest_num_of_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll analyse First Debate tweets dataset similar analysis would be applicable for second, third debates tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicate records and copying into another DF which we'll work on going forward\n",
    "tweets_DF = first.copy(deep = True)\n",
    "tweets_DF = tweets_DF.drop_duplicates()\n",
    "print(first.count())\n",
    "print(tweets_DF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DF.head()\n",
    "tweets_array = np.array(tweets_DF)\n",
    "tweets_array[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe of our own with the required columns for the analysis\n",
    "data = pd.DataFrame(data=[tweet[1] for tweet in tweets_array], columns=['text'])\n",
    "data['Date'] = np.array([tweet[0] for tweet in tweets_array])\n",
    "data['len']  = np.array([len(tweet[1]) for tweet in tweets_array])\n",
    "data['account_name']  = np.array([tweet[2] for tweet in tweets_array])\n",
    "data['Followers'] = np.array([tweet[3] for tweet in tweets_array])\n",
    "data['Likes']  = np.array([tweet[4] for tweet in tweets_array])\n",
    "data['Friends']  = np.array([tweet[5] for tweet in tweets_array])\n",
    "data['RTs']    = np.array([tweet[6] for tweet in tweets_array])\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see the mean length of all the tweets\n",
    "mean_length = np.mean(data['len'])\n",
    "mean_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see the tweets that has most retweets\n",
    "Rt_max  = np.max(data['RTs'])\n",
    "rt  = data[data.RTs == Rt_max].index[0]\n",
    "\n",
    "Like_max  = np.max(data['Likes'])\n",
    "like = data[data.Likes == Like_max].index[0]\n",
    "\n",
    "friends_max  = np.max(data['Friends'])\n",
    "friends  = data[data.Friends == friends_max].index[0]\n",
    "\n",
    "Followers_max  = np.max(data['Followers'])\n",
    "followers  = data[data.Followers == Followers_max].index[0]\n",
    "\n",
    "highest_num_of_tweets = np.max(data['account_name'].value_counts().sort_values(ascending = False))\n",
    "\n",
    "# Max RTs:\n",
    "print(\"The tweet with more retweets is: \\n{}\".format(data['text'][rt]))\n",
    "print(\"Number of retweets: {}\".format(Rt_max))\n",
    "print(\"{} characters\".format(data['len'][rt]))\n",
    "print(\"tweet done by {}. \\n\".format(data['account_name'][rt]))\n",
    "\n",
    "# Max RTs:\n",
    "print(\"The tweet with more likes is: \\n{}\".format(data['text'][like]))\n",
    "print(\"Number of likes: {}\".format(Like_max))\n",
    "print(\"{} characters\".format(data['len'][like]))\n",
    "print(\"tweet done by {}. \\n\".format(data['account_name'][like]))\n",
    "\n",
    "# Max Friends:\n",
    "print(\"The account with max friends is: \\n{}\".format(data['account_name'][friends]))\n",
    "print(\"Number of friends: {}\\n\".format(friends_max))\n",
    "\n",
    "# Max Followers:\n",
    "print(\"The account with max followers is: \\n{}\".format(data['account_name'][followers]))\n",
    "print(\"Number of followers: {}\\n\".format(Followers_max))\n",
    "\n",
    "# Highest # of tweets:\n",
    "print(\"The account with highest # of tweets: \\n{}\".format(data['account_name'][highest_num_of_tweets]))\n",
    "print(\"Number of tweets: {}\\n\".format(highest_num_of_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we've got some very interesting findings such as:\n",
    "1. Hillary Clinton has the highest # of followers \n",
    "2. Tweet that has got highest retweets is against Hillary\n",
    "though this may not be correct as we need to consider good chunk of data to come up a conclusion but interesting for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "Now we'll see the trend of tweets over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = np.max(data['Date'])\n",
    "min_date = np.min(data['Date'])\n",
    "print(max_date)\n",
    "print(min_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the time is very minute like 75 mins range. This could be a constraint to come upto a conclusion of the analysis.\n",
    "But as said earlier my goal is to write down how we can analyse data rather than conclusions, you can download whole data and apply the steps we're doing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create time series for data:\n",
    "\n",
    "tlen = pd.Series(data=data['len'].values, index=data['Date'])\n",
    "tfav = pd.Series(data=data['Likes'].values, index=data['Date'])\n",
    "tret = pd.Series(data=data['RTs'].values, index=data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the above created length variable over time:\n",
    "tlen.plot(figsize=(16,4), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likes vs retweets visualization:\n",
    "tfav.plot(figsize=(16,4), label=\"Likes\", legend=True)\n",
    "tret.plot(figsize=(16,4), label=\"Retweets\", legend=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = data['text']#taking only the text column\n",
    "text = np.array(text)#converting object to array\n",
    "text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're defining functions that takes tweet and cleans it to a form that helps us identify the sentiment of the tweet by passing it to textblob inbuilt method which assigns value -1 to 1 where 1 being positive and -1 being negative.\n",
    "Later, we'll assign this to positive, negative, neutral sentiments based on the value we get out of the textblob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "corpus = []\n",
    "def clean_tweet(tweet):\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) #removing uneccesary items like \n",
    "    #numbers, special characters that won't help us in predicting the sentiment of the tweet\n",
    "    tweet = review.lower()#making all the alphabets lower case\n",
    "    tweet = review.split()#splitting the word sentence into array of words to remove certain stopwords\n",
    "    ps = PorterStemmer()\n",
    "    tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]#removing stopwords in english language\n",
    "    clean_tweet = ' '.join(tweet)#joining the array of words back to form a cleaned tweet\n",
    "    return clean_tweet\n",
    "\n",
    "def analize_sentiment(tweet):#This function is to assign the sentiment to a tweet, textblob helps us do this with a simple method\n",
    "    \n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a column for the sentiment of the corresponding tweet\n",
    "data['Sentiment'] = np.array([ analize_sentiment(tweet) for tweet in text ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can have the classification of positive, negative, neutral tweets as mentioned earlier and the percentage of each\n",
    "    ptweets = [tweet for tweet in text if data['Sentiment'] == 'positive']\n",
    "    ntweets = [tweet for tweet in text if data['Sentiment'] == 'negative']\n",
    "    neutraltweets = [tweet for tweet in text if data['Sentiment'] == 'neutral']\n",
    "\n",
    "    print(\"Percentage of positive tweets: {}%\".format(len(ptweets)*100/len(text)))\n",
    "    print(\"Percentage of neutral tweets: {}%\".format(len(ntweets)*100/len(text)))\n",
    "    print(\"Percentage de negative tweets: {}%\".format(len(neutraltweets)*100/len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above analysis gives us a quick overview of the columns available and sentiment related to each tweet. Now, as the data source contains tweets during each of the presidential elections, I want to see how the sentiment of the people was with respect to Trump, Hillary by calculating sentiment of the tweets that mentioned only trump, only hillary, trump or hillary in the tweet. This'll give us some evidence of the people's view w.r.t to that candidate during each debate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis specific to each candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more sure that we're extracting the correct tweet repective to each candidate I've created a sample array that contains tweet specific to each of the candidate and see the result after applying the regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#took a sample of three tweets which contain only hillary, only trump and (trump and hillary) to see the behaviour of the next regex statements (as it's not possible to check if it's working properly with the whole list)\n",
    "x = [\"b'RT @itele : #debatenight \\\\ \\\\ u25ba Peu de pr \\\\ \\\\ xe9paration pour Donald Trump , \\\\ \\\\ xe0 quelques heures du d \\\\ \\\\ xe9bat entre les deux candidats \\\\ \\\\ xe0 la Maison Blanche htt \\\\ \\\\ u2026 '\",'b \" RT @The_Trump_Train : Social media will be flooded with lies from the MSM & amp ; Crooked Hillary\\'s surrogates . It\\'s crucial we tweet & amp ; retweet ea \\\\ \\\\ u2026 \"', 'b \" RT @HillaryClinton : Everything you need to know before watching tonight\\'s debate : https://t.co/Z4gcpSDfZa https://t.co/bymTtB2Et7 \"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets that contain trump\n",
    "r = re.compile(\".*trump\", re.M|re.I|re.IGNORECASE)\n",
    "trumptweets = list(filter(r.match, x))\n",
    "print(len(trumptweets))\n",
    "print(trumptweets[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets that contain hillary\n",
    "r = re.compile(\".*hillary\", re.M|re.I|re.IGNORECASE)\n",
    "hillarytweets = list(filter(r.match, x))\n",
    "print(len(hillarytweets))\n",
    "print(hillarytweets[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets that contain hillary but not trump\n",
    "r = re.compile('^(?!.*trump).*hillary.*$', re.M|re.I|re.IGNORECASE)\n",
    "onlyhillarytweets = list(filter(r.match, x))\n",
    "print(len(onlyhillarytweets))\n",
    "print(onlyhillarytweets[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets that contain trump but not hillary\n",
    "r = re.compile('^(?!.*hillary).*trump.*$', re.M|re.I|re.IGNORECASE)\n",
    "onlytrumptweets = list(filter(r.match, x))\n",
    "print(len(onlytrumptweets))\n",
    "print(onlytrumptweets[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can conclude that the regex statements are working properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further Analysis\n",
    "We can create the above dataframes with the whole data similarly by replacing 'x' with the 'text' and calculate the sentiment for each candidate and the number of positive, negative, neutral tweets to generalize the sentiment of each debate by passing the corresponding debate dataset. As we've over 3 million records for each debate if you could import all of them we can also see the trend of the sentiment over time that's before the debate, during and after debate to say if that debate is a plus or minus for a particular candidate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
